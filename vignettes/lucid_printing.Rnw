% lucid_printing.Rnw
% Time-stamp: <09 Jan 2015 08:53:25 c:/x/rpack/lucid/vignettes/lucid_printing.Rnw>

% \VignetteEngine{knitr::knitr}
% \VignetteIndexEntry{Lucid printing}

\documentclass[12pt]{article}
% Note, 11 pt fails with a missing font unless we have \usepackage{ae}

\usepackage{color}
\definecolor{navy}{rgb}{0,0,0.5}
\definecolor{maroon}{rgb}{0.6,0.0,0.0}
\definecolor{lightgray}{rgb}{0.7,0.7,0.7}

\usepackage[colorlinks,citecolor=navy,linkcolor=navy,urlcolor=navy]{hyperref}

%\usepackage{url}     % Don't break long URLs
\raggedright         % Don't right-justify
\usepackage{parskip} % Extra vertical space between paragraphs

\usepackage{enumitem}
\setlist{nolistsep}  % Reduce spacing between list items

\usepackage[left=.5in,top=.5in,right=.5in,bottom=1in]{geometry}

\newcommand{\code}[1]{\texttt{\textcolor{maroon}{#1}}}

% Header & footer
\usepackage{titling} % copy \title{} into \thetitle
\usepackage{fancyhdr}
\pagestyle{fancy}
\renewcommand{\headrule}{}  % no line between header and body
\renewcommand{\footrule}{}  % no line between body and footer
\fancyhf{} % clear header and footer fields
\fancyfoot[L]{\small \color{lightgray} \thetitle}
\fancyfoot[R]{\small \thepage}

\usepackage[round]{natbib}
% plainnat to include url's in bibliography.  Authors first name is first
% apalike does NOT handle urls
\bibliographystyle{plainnat}

\renewcommand{\familydefault}{\sfdefault} % Use arial everywhere
% Load this AFTER arial and it will redefine tt family to use inconsolata.
\IfFileExists{zi4.sty}{\usepackage[noupquote]{zi4}}{\usepackage{inconsolata}}

% ----------------------------------------------------------------------------

\begin{document}

\title{Lucid printing}
\author{Kevin Wright}
\maketitle
\thispagestyle{fancy}

% Setup stuff.
<<SETUP, echo=FALSE, results="hide">>=
library("knitr")
opts_chunk$set(fig.align="center", fig.width=7, fig.height=7)
options(width=90)
@
% ----------------------------------------------------------------------------

\section{Abstract}

The \code{lucid} package provides a function for printing vectors of floating
point numbers in a human-friendly format.  An application is presented for
printing of variance components from mixed models.

% ----------------------------------------------------------------------------
\section{Intro}

Numerical output from R is often in scientific notation, which can make it
difficult to quickly glance at numbers and understand the relative sizes
of the numbers.  This not a new phenomenon.  Before R had been created,
\cite[351-352]{finney1988data} had this to say about numerical output:
\begin{quote}
Certainly, in initiating analyses by standard software or in writing one's
own software, the aim should be to have output that is easy to read and easily
intelligible to others. ... Especially undesirable is the so-called
'scientific notation' for numbers in which every number is shown as a value
between 0.0 and 1.0 with a power of 10 by which it must be multiplied. For
example:
\begin{verbatim}
0.1234E00 is 0.1234
0.1234E02 is 12.34
0.1234E-1 is 0.01234
\end{verbatim}
This is an abomination which obscures the comparison of related quantities;
tables of means or of analyses of variance become very difficult to read. It
is acceptable as a default when a value is unexpectedly very much larger or
smaller than its companions, but its appearance as standard output denotes
either lazy programming or failure to use good software properly. Like
avoidance of 'E', neat arrangement of output values in columns, with decimal
points on a vertical line, requires extra effort by a programmer but should be
almost mandatory for any software that is to be used often.
\end{quote}

One recommendation for improving the display of tables of numbers
is to round numbers to 2 \citep{wainer1997improving}
or 3 \citep{feinberg2011extracting} digits.
\cite{feinberg2011extracting} give the following justification for aggresive
rounding:
\begin{enumerate}
\item Humans cannot comprehend more than three digits very easily.
\item We almost never care about accuracy of more than three digits.
\item We can only rarely justify more than three digits of accuracy statistically.
\end{enumerate}

In R, using the \code{round} and \code{signif} functions can be used to round
to 3 digits of accuracy, but those functions
can still print results in scientific notation
and leave much to be desired.
The \code{lucid} package provides functions to improve the presentation of
floating point numbers in a clear (or lucid)
way that makes interpretation of the numbers immediately apparent.

Consider the following vector of coefficients from a fitted model:
<<echo=FALSE>>=
df1 <- data.frame(effect=c(-13.5, 4.5,  24.5, 6.927792e-14, -1.75,
                    16.5, 113.5000))
rownames(df1) <- c("A","B","C","C1","C2","D","(Intercept)")
print(df1)
@
Which coeficient is basically zero?  How large is the intercept?

Both questions can be answered using the output shown above, but it
takes too much effort to answer the questions.  Now examine the same
vector of coefficients with prettier formatting:
<<message=FALSE>>=
require("lucid")
options(digits=7) # knitr defaults to 4, R console uses 7
lucid(df1)
@
Which coeficient is basically zero?  How large is the intercept?

Printing the numbers with the \code{lucid} function has made the
questions much easier to answer.

The sequence of steps used by \code{lucid} to format and
print the output is.
\begin{enumerate}
\item{Zap small numbers to zero}
\item{Round using 3 significant digits (user controllable option) }
\item{Drop trailing zeros}
\item{Align numbers at the decimal point}
\end{enumerate}

The \code{lucid} package contains a generic function \code{lucid} with
specific methods for numeric vectors, data frames, and lists.
The method for data frames applies formatting to each
numeric column and leaves other columns unchanged.
The \code{lucid} function is primarily a {\it formatting} function, the
results of which are passed to the regular \code{print} functions.

% ----------------------------------------------------------------------------

\section{Example: Antibiotic effectiveness}

\cite{wainer2009pictures} present data published by Will Burtin in 1951
on the effectiveness of antibiotics against 16 types of bacteria.
The data is included in the \code{lucid} package as a dataframe
called \code{antibiotic}.  The default view of this data is:
<<>>=
print(antibiotic)
@
Due to the wide range in magnitude of the values, nearly half of the
floating-point numbers in the default view contain trailing zeros
after the decimal, which adds significant clutter and impedes
interpretation.  The \code{lucid} display of the data is:
<<>>=
lucid(antibiotic)
@
The \code{lucid} display is dramatically simplified, providing a
clear picture of the effectiveness of the antibiotics against
bacteria.  This view of the data matches exactly the appearance of
Table 1 in \cite{wainer2009pictures}.

A stem-and-leaf plot is a semi-graphical display of data, in that the {\it
positions} of the numbers create a display similar to a histogram.  In a
similar manner, the \code{lucid} output is a semi-graphical view of the data.
The figure below shows a dotplot of the penicillin values on a reverse log10
scale.  The values are also shown along the right axis in \code{lucid} format.
Note the similarity in the overall shape of the dots and the positions of the
left-most significant digit in the numerical value.
% Also, as noted by \cite{gelman2011tables}, the amount of ink in printing
% the significant digit has a surprisingly large correlation with the value
% of the digit, increasing the information in the semi-graphical view
% from \code{lucid} printing.
<<echo=FALSE, message=FALSE, fig.height=4, fig.width=6>>=
require(lattice)
anti=antibiotic # make a copy of the data to reverse the levels
anti$bacteria <- factor(anti$bacteria, levels=rev(anti$bacteria))

cust <- myyscale.component <- function(...) {  #Custom y-scale function component
  ans <- yscale.components.default(...)
  ans$right <- ans$left
  foo <- ans$right$labels$at
  ans$right$labels$labels <- rev(c(" 870    ","   1    ","   0.001","   0.005"," 100    ",
                               "850    "," 800    ","   3    "," 850    ","   1    ",
                               " 10    ","   0.007", "  0.03 ","   1    ",
                               "  0.001","   0.005"))
  return(ans)
}

dotplot(bacteria~ -log10(penicillin), anti,
        cex=1, xlim=c(-4,4), #xlab="variance component (log10 scale)",
        scales=list(x=list(at= c(-2,0,2),
                      labels=c('100','1','.01')),
          y=list(relation="free", fontfamily='mono')), # 'free' required for 2nd axis
        yscale.components=cust,
        #this creates more space on the right hand side of the plot
        par.settings=list(layout.widths=list(left.padding=10,right.padding=10))
        )

@

% ----------------------------------------------------------------------------

\section{Application to mixed models}

During the process of iterative fitting of mixed models, it is often useful to
compare fits of different models to data, for example using loglikelihood or
AIC values, or with the help of resiudal plots.  It can also be very
informative to inspect the estimated values of variance components.

To that end, the generic \code{VarCorr} function found in the \code{nlme}
\citep{pinheiro2014nlme} and \code{lme4} \citep{bates2014lme4} packages can be
used to print variance estimates from fitted models.  The \code{VarCorr}
function is not available for models obtained using the \code{asreml}
\citep{butler2009asreml} package.

The \code{lucid} package provides a generic function called \code{vc} that
provides a unified interface for extracting the variance components from
fitted models obtained from the \code{nlme}, \code{lme4}, and \code{asreml}
packages.  The \code{vc} function has methods specific to each package that
make it easy to extract the estimated variances and correlations from fitted
models and formats the results using the \code{lucid} function.

\cite{pearce1988manual} suggest showing four significant digits for the error
mean square and two decimal places digits for $F$ values.  The \code{lucid}
function uses a similar philosophy, presenting the variances with four
significant digits and \code{asreml} $Z$ statistics with two significant
digits.

The following simple example illustrates use of the \code{vc} function for
identical models in the \code{nlme}, \code{lme4}, and \code{asreml} packages.
The travel times of ultrasonic waves in six steel rails was modeled as an
overall mean, a random effect for each rail, and a random residual.

<<message=FALSE>>=
require("nlme")
data(Rail)
mn <- lme(travel~1, random=~1|Rail, data=Rail)
vc(mn)

require("lme4")
m4 <- lmer(travel~1 + (1|Rail), data=Rail)
vc(m4)

# require("asreml")
# ma <- asreml(travel~1, random=~Rail, data=Rail)
# vc(ma)
##         effect component std.error z.ratio constr
##  Rail!Rail.var    615.3      392.6     1.6    pos
##     R!variance     16.17       6.6     2.4    pos
@

While the \code{lucid} function is primarily a formatting function and uses
the standard \code{print} functions in R, the \code{vc} function defines an
additional class for the value of the function and has dedicated \code{print}
methods for the class.  This was done to allow additional formatting of the
results.

The second, more complex example is based on a paper by \cite{federer2003proc}
in which orthogonal polynomials are used to model trends along the rows and
columns of a field experiment.  The data are available in the \code{agridat}
package \cite{wright2014agridat} as the \code{federer.diagcheck} data frame.
The help page for the data shows how to reproduce the analysis of
\cite{federer2003proc}.  When using the \code{lme4} package to reproduce the
analysis, two different optimizers are available.  Do the two different
optimizers lead to similar estimated variances?

In the output below, the first column identifies terms in the model, the next
two columns are the variance and standard deviation from the \texttt{bobyqa}
optimizer, while the final two columns are from the \texttt{NelderMead} optimizer.

\pagebreak

The default output printing is shown first.

<<echo=FALSE>>=
# Results are from lme4_1.1-7, as.data.frame(VarCorr(m2b))
d1 <- structure(list(grp = c("new.gen", "one", "one.1", "one.2", "one.3",
"one.4", "one.5", "one.6", "one.7", "one.8", "one.9", "one.10",
"one.11", "one.12", "one.13", "Residual"), var1 = c("(Intercept)",
"r1:c3", "r1:c2", "r1:c1", "c8", "c6", "c4", "c3", "c2", "c1",
"r10", "r8", "r4", "r2", "r1", NA), var2 = c(NA_character_, NA_character_,
NA_character_, NA_character_, NA_character_, NA_character_, NA_character_,
NA_character_, NA_character_, NA_character_, NA_character_, NA_character_,
NA_character_, NA_character_, NA_character_, NA_character_),
    vcov = c(2869.44692139271, 5531.57239635089, 58225.767835444,
    128004.156092455, 6455.74953933247, 1399.72937329085, 1791.65071661348,
    2548.88470543732, 5941.79076230161, 0, 1132.95013713932,
    1355.22907294114, 2268.72957045473, 241.789424531994, 9199.9021721834,
    4412.1096176349), sdcor = c(53.5672187199663, 74.3745413185916,
    241.300161283502, 357.7766846686, 80.3476791160297, 37.4129572914365,
    42.3278952537623, 50.4864804223598, 77.0830121511972, 0,
    33.6593246684974, 36.8134360382339, 47.6311827530529, 15.5495795612613,
    95.9161205021523, 66.4237127661116)), .Names = c("grp", "var1",
    "var2", "vcov", "sdcor"), row.names = c(NA, -16L), class = "data.frame")

d2 <- structure(list(grp = c("new.gen", "one", "one.1", "one.2", "one.3",
"one.4", "one.5", "one.6", "one.7", "one.8", "one.9", "one.10",
"one.11", "one.12", "one.13", "Residual"), var1 = c("(Intercept)",
"r1:c3", "r1:c2", "r1:c1", "c8", "c6", "c4", "c3", "c2", "c1",
"r10", "r8", "r4", "r2", "r1", NA), var2 = c(NA_character_, NA_character_,
NA_character_, NA_character_, NA_character_, NA_character_, NA_character_,
NA_character_, NA_character_, NA_character_, NA_character_, NA_character_,
NA_character_, NA_character_, NA_character_, NA_character_),
    vcov = c(3228.41890564251, 7688.13916836557, 69747.5508913552,
    107427.043198654, 6787.00354507896, 1636.12771714548, 12268.4603217744,
    2686.30159414561, 7644.72994565782, 0.00122514315152732,
    1975.50482871438, 1241.42852423718, 2811.24084391436, 928.227473340838,
    10363.5849610346, 4126.83169047631), sdcor = c(56.8191772700249,
    87.6820344675326, 264.097616216722, 327.760649252856, 82.3832722406616,
    40.4490756031022, 110.763081944186, 51.8295436420735, 87.4341463368736,
    0.0350020449620779, 44.4466514904597, 35.23391156595, 53.02113582256,
    30.4668257838069, 101.801694293536, 64.2404210017051)), .Names = c("grp",
"var1", "var2", "vcov", "sdcor"), row.names = c(NA, -16L), class = "data.frame")
out <- cbind(d1[, c(2,4,5)], sep="|",d2[,4:5])
names(out) <- c('term','vcov-bo','sdcor-bo','sep','vcov-ne','sdcor-ne')
@
<<>>=
print(out)
@
How similar are the variance estimates obtained from the two optimization
methods? It is difficult to compare the results due to the clutter of extra
digits, and because of some quirks in the way R formats the
output.  The variances in column 2 are shown in non-scientific format, while
the variances in column 5 are shown in scientific format.  The standard
deviations are shown with 5 decimal places in column 3 and 8 decimal places in
column 6.  (All numbers were stored with 15 digits of precision.)

The \code{lucid} function is now used to show the results in the manner of the
\code{vc} function.
<<>>=
lucid(out, dig=4)
@
The formatting of the variance columns is consistent as is the formatting of
the standard deviation columns.  Fewer digits are shown.  It is easy to
compare the columns and see that the two optimizers are giving quite different
answers.


% Look at the raw/lucid table of variance components below.
% Which are the smallest/largest/significant variance components?
% <<echo=TRUE>>=
% df2 <- data.frame(effect=c('hyb','region','region:loc','hyb:region','yr','hyb:yr','region:yr','residual'),
%                   component=c(10.9,277,493,1.30E-04,126,22.3,481,268),
%                   std.error=c(4.40,166,26.1,1.58E-06,119,4.50,108,3.25),
%                   z.ratio=c(2.471,1.669,18.899,82.242,1.060,4.951,4.442,82.242),
%                   constraint=c('pos','pos','pos','bnd','pos','pos','pos','pos'))
% print(df2)
% @
% <<echo=TRUE>>=
% lucid(df2)
% @

%Note the similarity in overall shape of the positions of the leftmost
%significant digit in the 'component' column of the lucid output and
%the dotplot of the components on a log10 scale.  See \cite{gelman2011tables}.
%<<fig.height=4>>=
%df2$effect <- factor(df2$effect, levels=rev(df2$effect))
%require(lattice)
%dotplot(effect~ log10(component), data=df2,
%        cex=1, xlim=c(3,-4), xlab="variance component (log10 scale)",
%        scales=list(x=list(lab=rev(c('1000','100','10','1','.1','.01','.001'
%,'.0001')))))
%@


% ----------------------------------------------------------------------------

\section{Summary}


\section{Acknowledgements}

Thanks to Deanne Wright for a helpful review of this paper.

\section{Appendix}

Session information:
<<finish, echo=FALSE, results="asis">>=
toLatex(sessionInfo(), locale=FALSE)
@

\bibliography{lucid}
\end{document}
